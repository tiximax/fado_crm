# FADO CRM - Advanced Machine Learning Engine # He thong AI thong minh cho du bao va phat hien bat thuong from typing import Dict, List, Tuple, Optional, Any from datetime import datetime, timedelta import json import asyncio from sqlalchemy.orm import Session from sqlalchemy import func, and_, desc, extract, text from models import DonHang, KhachHang, SanPham, ChiTietDonHang, TrangThaiDonHang, LoaiKhachHang from database import get_db # Optional ML dependencies - graceful degradation try: import numpy as np import pandas as pd from scipy import stats from scipy.stats import zscore from sklearn.preprocessing import StandardScaler, MinMaxScaler from sklearn.cluster import KMeans, DBSCAN from sklearn.ensemble import IsolationForest from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, mean_squared_error from sklearn.decomposition import PCA import warnings warnings.filterwarnings('ignore') ML_DEPENDENCIES_AVAILABLE = True except ImportError: ML_DEPENDENCIES_AVAILABLE = False np = None pd = None class AdvancedMLEngine: """ Advanced Machine Learning Engine cho FADO CRM Features: - Time-series forecasting cho demand prediction - Anomaly detection cho fraud prevention - Customer behavior clustering - Advanced risk scoring - Seasonal trend analysis """ def __init__(self): self.models_trained = {} self.feature_scalers = {} self.ml_available = ML_DEPENDENCIES_AVAILABLE async def get_time_series_data(self, db: Session, days_back: int = 180) -> Dict[str, Any]: """ Thu thap du lieu time series cho forecasting """ if not self.ml_available: return {"error": "ML dependencies not available", "available": False} end_date = datetime.now() start_date = end_date - timedelta(days=days_back) # Daily sales data daily_sales = db.query( func.date(DonHang.ngay_tao).label('date'), func.count(DonHang.id).label('order_count'), func.sum(DonHang.tong_tien).label('total_revenue'), func.avg(DonHang.tong_tien).label('avg_order_value') ).filter( and_( DonHang.ngay_tao >= start_date, DonHang.ngay_tao <= end_date, DonHang.trang_thai != TrangThaiDonHang.HUY ) ).group_by(func.date(DonHang.ngay_tao)).all() # Product category trends category_trends = db.query( func.date(DonHang.ngay_tao).label('date'), SanPham.danh_muc.label('category'), func.sum(ChiTietDonHang.so_luong).label('quantity'), func.sum(ChiTietDonHang.thanh_tien).label('revenue') ).join(ChiTietDonHang).join(SanPham).filter( and_( DonHang.ngay_tao >= start_date, DonHang.ngay_tao <= end_date, DonHang.trang_thai != TrangThaiDonHang.HUY ) ).group_by(func.date(DonHang.ngay_tao), SanPham.danh_muc).all() # Customer acquisition trends customer_trends = db.query( func.date(KhachHang.ngay_tao).label('date'), func.count(KhachHang.id).label('new_customers') ).filter( and_( KhachHang.ngay_tao >= start_date, KhachHang.ngay_tao <= end_date ) ).group_by(func.date(KhachHang.ngay_tao)).all() return { "available": True, "period": {"start": start_date, "end": end_date, "days": days_back}, "daily_sales": [ { "date": item.date.isoformat(), "order_count": item.order_count, "total_revenue": float(item.total_revenue or 0), "avg_order_value": float(item.avg_order_value or 0) } for item in daily_sales ], "category_trends": [ { "date": item.date.isoformat(), "category": item.category, "quantity": item.quantity, "revenue": float(item.revenue or 0) } for item in category_trends ], "customer_trends": [ { "date": item.date.isoformat(), "new_customers": item.new_customers } for item in customer_trends ] } async def forecast_demand(self, db: Session, forecast_days: int = 30) -> Dict[str, Any]: """ Time-series forecasting cho demand prediction """ if not self.ml_available: return {"error": "ML dependencies not available", "available": False} try: # Get historical data ts_data = await self.get_time_series_data(db, days_back=180) if not ts_data.get("available"): return ts_data daily_sales = ts_data["daily_sales"] if len(daily_sales) < 30: return {"error": "Insufficient historical data", "required_days": 30, "available_days": len(daily_sales)} # Convert to DataFrame df = pd.DataFrame(daily_sales) df['date'] = pd.to_datetime(df['date']) df = df.set_index('date').sort_index() # Fill missing dates with 0s full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D') df = df.reindex(full_range, fill_value=0) forecasts = {} # Forecast order count order_forecast = self._simple_forecast(df['order_count'].values, forecast_days) forecasts['order_count'] = order_forecast # Forecast revenue revenue_forecast = self._simple_forecast(df['total_revenue'].values, forecast_days) forecasts['total_revenue'] = revenue_forecast # Generate future dates last_date = df.index.max() future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=forecast_days, freq='D') # Seasonal patterns analysis seasonal_analysis = self._analyze_seasonal_patterns(df) # Confidence intervals confidence_intervals = self._calculate_confidence_intervals(df, forecasts) return { "available": True, "forecast_period": forecast_days, "predictions": [ { "date": date.isoformat(), "predicted_orders": max(0, int(forecasts['order_count']['predictions'][i])), "predicted_revenue": max(0, round(forecasts['total_revenue']['predictions'][i], 2)), "confidence_score": round(forecasts['order_count']['confidence'] * 100, 1), "lower_bound_orders": max(0, int(confidence_intervals['order_count']['lower'][i])), "upper_bound_orders": max(0, int(confidence_intervals['order_count']['upper'][i])), "lower_bound_revenue": max(0, round(confidence_intervals['total_revenue']['lower'][i], 2)), "upper_bound_revenue": max(0, round(confidence_intervals['total_revenue']['upper'][i], 2)) } for i, date in enumerate(future_dates) ], "seasonal_patterns": seasonal_analysis, "model_performance": { "order_count_mae": round(forecasts['order_count']['mae'], 2), "revenue_mae": round(forecasts['total_revenue']['mae'], 2), "overall_confidence": round((forecasts['order_count']['confidence'] + forecasts['total_revenue']['confidence']) / 2 * 100, 1) }, "business_insights": await self._generate_forecast_insights(db, forecasts, seasonal_analysis) } except Exception as e: return {"error": f"Forecasting failed: {str(e)}", "available": False} def _simple_forecast(self, data: np.ndarray, periods: int) -> Dict[str, Any]: """ Simple but effective forecasting using linear regression + moving average """ if len(data) < 7: return {"predictions": [0] * periods, "confidence": 0.0, "mae": 0.0} # Prepare features: trend + seasonal X = np.arange(len(data)).reshape(-1, 1) # Create additional features X_extended = np.column_stack([ X.flatten(), # Linear trend np.sin(2 * np.pi * X.flatten() / 7), # Weekly seasonality np.sin(2 * np.pi * X.flatten() / 30.44), # Monthly seasonality np.cos(2 * np.pi * X.flatten() / 7), # Weekly seasonality (cosine) np.cos(2 * np.pi * X.flatten() / 30.44) # Monthly seasonality (cosine) ]) # Train linear regression model model = LinearRegression() model.fit(X_extended, data) # Calculate MAE on recent data recent_data = data[-30:] if len(data) >= 30 else data recent_X = X_extended[-len(recent_data):] predictions_recent = model.predict(recent_X) mae = mean_absolute_error(recent_data, predictions_recent) # Confidence based on R-squared and consistency train_pred = model.predict(X_extended) r_squared = 1 - (np.sum((data - train_pred) ** 2) / np.sum((data - np.mean(data)) ** 2)) confidence = max(0.1, min(0.95, r_squared)) # Generate future predictions future_X = np.arange(len(data), len(data) + periods) future_X_extended = np.column_stack([ future_X, np.sin(2 * np.pi * future_X / 7), np.sin(2 * np.pi * future_X / 30.44), np.cos(2 * np.pi * future_X / 7), np.cos(2 * np.pi * future_X / 30.44) ]) future_predictions = model.predict(future_X_extended) # Apply moving average smoothing to recent trend if len(data) >= 7: recent_avg = np.mean(data[-7:]) trend_adj = np.linspace(0, 0.3, periods) future_predictions = (future_predictions * (1 - trend_adj)) + (recent_avg * trend_adj) return { "predictions": future_predictions.tolist(), "confidence": confidence, "mae": mae, "model_score": r_squared } def _analyze_seasonal_patterns(self, df: pd.DataFrame) -> Dict[str, Any]: """ Phan tich seasonal patterns trong du lieu """ patterns = {} # Day of week patterns df['dow'] = df.index.day_of_week dow_avg = df.groupby('dow')[['order_count', 'total_revenue']].mean() patterns['day_of_week'] = { "peak_day": int(dow_avg['order_count'].idxmax()), "lowest_day": int(dow_avg['order_count'].idxmin()), "peak_day_name": ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'][int(dow_avg['order_count'].idxmax())], "average_orders_by_day": dow_avg['order_count'].round(1).to_dict() } # Monthly patterns df['month'] = df.index.month monthly_avg = df.groupby('month')[['order_count', 'total_revenue']].mean() patterns['monthly'] = { "peak_month": int(monthly_avg['order_count'].idxmax()), "lowest_month": int(monthly_avg['order_count'].idxmin()), "average_orders_by_month": monthly_avg['order_count'].round(1).to_dict() } return patterns def _calculate_confidence_intervals(self, df: pd.DataFrame, forecasts: Dict) -> Dict[str, Any]: """ Tinh confidence intervals cho predictions """ intervals = {} for metric in ['order_count', 'total_revenue']: predictions = forecasts[metric]['predictions'] mae = forecasts[metric]['mae'] # Simple confidence interval based on MAE margin = mae * 1.96 # 95% confidence interval approximation intervals[metric] = { 'lower': [max(0, pred - margin) for pred in predictions], 'upper': [pred + margin for pred in predictions] } return intervals async def _generate_forecast_insights(self, db: Session, forecasts: Dict, seasonal: Dict) -> List[str]: """ Tao business insights tu forecast results """ insights = [] # Revenue trend insight predicted_revenues = forecasts['total_revenue']['predictions'] avg_revenue = np.mean(predicted_revenues) if len(predicted_revenues) >= 7: week1_avg = np.mean(predicted_revenues[:7]) week4_avg = np.mean(predicted_revenues[-7:]) if len(predicted_revenues) >= 28 else week1_avg if week4_avg > week1_avg * 1.1: insights.append(" Doanh thu du kien tang truong manh trong thang toi") elif week4_avg < week1_avg * 0.9: insights.append(" Doanh thu co xu huong giam - can co chien luoc kich thich") else: insights.append(" Doanh thu du kien on dinh trong thang toi") # Seasonal insights if 'day_of_week' in seasonal: peak_day = seasonal['day_of_week']['peak_day_name'] insights.append(f" {peak_day} la ngay co don hang cao nhat trong tuan") # Volume insights predicted_orders = forecasts['order_count']['predictions'] total_predicted_orders = sum(predicted_orders) insights.append(f" Du kien co {int(total_predicted_orders)} don hang trong {len(predicted_orders)} ngay toi") return insights async def detect_anomalies(self, db: Session, analysis_days: int = 90) -> Dict[str, Any]: """ Anomaly Detection cho fraud prevention va risk management """ if not self.ml_available: return {"error": "ML dependencies not available", "available": False} try: end_date = datetime.now() start_date = end_date - timedelta(days=analysis_days) # Get detailed transaction data transactions = db.query( DonHang.id, DonHang.ma_don_hang, DonHang.khach_hang_id, DonHang.tong_tien, DonHang.ngay_tao, DonHang.trang_thai, KhachHang.ten.label('customer_name'), KhachHang.email, KhachHang.so_dien_thoai, KhachHang.loai.label('customer_type'), func.count(ChiTietDonHang.id).label('item_count') ).join(KhachHang).outerjoin(ChiTietDonHang).filter( and_( DonHang.ngay_tao >= start_date, DonHang.ngay_tao <= end_date, DonHang.trang_thai != TrangThaiDonHang.HUY ) ).group_by(DonHang.id).all() if len(transactions) < 10: return {"error": "Insufficient transaction data", "available": False} # Convert to DataFrame for analysis df = pd.DataFrame([{ 'order_id': t.id, 'order_code': t.ma_don_hang, 'customer_id': t.khach_hang_id, 'amount': float(t.tong_tien), 'timestamp': t.ngay_tao, 'status': t.trang_thai.value, 'customer_name': t.customer_name, 'email': t.email, 'phone': t.so_dien_thoai, 'customer_type': t.customer_type.value, 'item_count': t.item_count, 'hour': t.ngay_tao.hour, 'day_of_week': t.ngay_tao.weekday(), 'is_weekend': t.ngay_tao.weekday() >= 5 } for t in transactions]) anomalies = await self._detect_multiple_anomaly_types(db, df) return { "available": True, "analysis_period": {"days": analysis_days, "start": start_date, "end": end_date}, "total_transactions": len(transactions), "anomalies_detected": sum(len(anomalies[key]) for key in anomalies), "anomaly_types": anomalies, "risk_assessment": await self._assess_overall_risk(anomalies), "recommendations": await self._generate_anomaly_recommendations(anomalies) } except Exception as e: return {"error": f"Anomaly detection failed: {str(e)}", "available": False} async def _detect_multiple_anomaly_types(self, db: Session, df: pd.DataFrame) -> Dict[str, List]: """ Phat hien nhieu loai anomaly khac nhau """ anomalies = { "amount_anomalies": [], "frequency_anomalies": [], "time_anomalies": [], "customer_behavior_anomalies": [], "pattern_anomalies": [] } # 1. Amount-based anomalies (Statistical outliers) if len(df) > 5: z_scores = np.abs(zscore(df['amount'])) amount_outliers = df[z_scores > 3] for _, row in amount_outliers.iterrows(): anomalies["amount_anomalies"].append({ "order_id": row['order_id'], "order_code": row['order_code'], "customer_id": row['customer_id'], "amount": row['amount'], "z_score": float(z_scores[row.name]), "reason": "Unusual transaction amount", "severity": "HIGH" if z_scores[row.name] > 4 else "MEDIUM" }) # 2. Customer frequency anomalies customer_freq = df.groupby('customer_id').size() freq_outliers = customer_freq[customer_freq > customer_freq.quantile(0.99)] for customer_id, freq in freq_outliers.items(): customer_orders = df[df['customer_id'] == customer_id] anomalies["frequency_anomalies"].append({ "customer_id": customer_id, "order_count": freq, "total_amount": customer_orders['amount'].sum(), "avg_amount": customer_orders['amount'].mean(), "reason": f"Unusually high order frequency: {freq} orders", "severity": "HIGH" if freq > customer_freq.quantile(0.999) else "MEDIUM", "orders": customer_orders['order_code'].tolist()[:5] # First 5 orders }) # 3. Time-based anomalies (unusual hours) unusual_hours = [0, 1, 2, 3, 4, 5] # Very early morning late_night_orders = df[df['hour'].isin(unusual_hours)] if len(late_night_orders) > 0: for _, row in late_night_orders.iterrows(): anomalies["time_anomalies"].append({ "order_id": row['order_id'], "order_code": row['order_code'], "timestamp": row['timestamp'].isoformat(), "hour": row['hour'], "amount": row['amount'], "reason": f"Order placed at unusual hour: {row['hour']}:00", "severity": "LOW" }) # 4. Customer behavior anomalies (using Isolation Forest) if len(df) > 10: feature_cols = ['amount', 'item_count', 'hour', 'day_of_week'] features = df[feature_cols].fillna(0) # Normalize features scaler = StandardScaler() features_scaled = scaler.fit_transform(features) # Apply Isolation Forest iso_forest = IsolationForest(contamination=0.1, random_state=42) outlier_labels = iso_forest.fit_predict(features_scaled) behavior_outliers = df[outlier_labels == -1] for _, row in behavior_outliers.iterrows(): anomalies["customer_behavior_anomalies"].append({ "order_id": row['order_id'], "order_code": row['order_code'], "customer_id": row['customer_id'], "amount": row['amount'], "item_count": row['item_count'], "reason": "Unusual customer behavior pattern", "severity": "MEDIUM", "behavior_score": float(iso_forest.decision_function(features_scaled[row.name].reshape(1, -1))[0]) }) # 5. Pattern anomalies (rapid successive orders) df_sorted = df.sort_values(['customer_id', 'timestamp']) rapid_orders = [] for customer_id in df['customer_id'].unique(): customer_orders = df_sorted[df_sorted['customer_id'] == customer_id] if len(customer_orders) > 1: time_diffs = customer_orders['timestamp'].diff() rapid_mask = time_diffs < timedelta(minutes=5) if rapid_mask.any(): rapid_orders.extend(customer_orders[rapid_mask].to_dict('records')) for order in rapid_orders: anomalies["pattern_anomalies"].append({ "order_id": order['order_id'], "order_code": order['order_code'], "customer_id": order['customer_id'], "amount": order['amount'], "reason": "Rapid successive orders within 5 minutes", "severity": "HIGH" }) return anomalies async def _assess_overall_risk(self, anomalies: Dict) -> Dict[str, Any]: """ Danh gia risk tong the tu cac anomalies """ total_anomalies = sum(len(anomalies[key]) for key in anomalies) high_severity = sum(len([a for a in anomalies[key] if a.get('severity') == 'HIGH']) for key in anomalies) if total_anomalies == 0: risk_level = "LOW" risk_score = 0 elif high_severity > 5: risk_level = "HIGH" risk_score = min(100, 60 + high_severity * 8) elif total_anomalies > 10: risk_level = "MEDIUM" risk_score = min(100, 30 + total_anomalies * 3) else: risk_level = "LOW" risk_score = total_anomalies * 5 return { "risk_level": risk_level, "risk_score": risk_score, "total_anomalies": total_anomalies, "high_severity_count": high_severity, "categories_affected": len([k for k in anomalies if len(anomalies[k]) > 0]) } async def _generate_anomaly_recommendations(self, anomalies: Dict) -> List[str]: """ Tao recommendations based on detected anomalies """ recommendations = [] if len(anomalies["amount_anomalies"]) > 0: recommendations.append(" Review high-value transactions for potential fraud") recommendations.append(" Consider implementing transaction amount limits") if len(anomalies["frequency_anomalies"]) > 0: recommendations.append(" Monitor customers with unusual ordering patterns") recommendations.append(" Contact high-frequency customers to verify authenticity") if len(anomalies["time_anomalies"]) > 0: recommendations.append(" Consider limiting order placement during unusual hours") recommendations.append(" Implement additional verification for off-hours orders") if len(anomalies["customer_behavior_anomalies"]) > 0: recommendations.append(" Investigate customers with unusual behavior patterns") recommendations.append(" Enhance customer profiling for better detection") if len(anomalies["pattern_anomalies"]) > 0: recommendations.append(" Implement cooldown periods between orders") recommendations.append(" Flag rapid successive orders for manual review") if not any(len(anomalies[key]) > 0 for key in anomalies): recommendations.append(" No significant anomalies detected - system appears healthy") recommendations.append(" Continue regular monitoring for early detection") return recommendations async def generate_ml_insights(self, db: Session) -> Dict[str, Any]: """ Tao comprehensive ML insights cho business intelligence """ if not self.ml_available: return {"error": "ML dependencies not available", "available": False} try: # Get forecasting insights forecast_result = await self.forecast_demand(db, forecast_days=14) # Get anomaly detection insights anomaly_result = await self.detect_anomalies(db, analysis_days=30) # Customer segmentation using ML customer_segments = await self._advanced_customer_segmentation(db) # Product performance clustering product_clusters = await self._product_performance_clustering(db) return { "available": True, "generated_at": datetime.now().isoformat(), "forecast_insights": forecast_result if forecast_result.get("available") else None, "anomaly_insights": anomaly_result if anomaly_result.get("available") else None, "customer_segmentation": customer_segments, "product_clustering": product_clusters, "overall_health_score": await self._calculate_ml_health_score( forecast_result, anomaly_result, customer_segments, product_clusters ), "ai_recommendations": await self._generate_comprehensive_recommendations( forecast_result, anomaly_result, customer_segments, product_clusters ) } except Exception as e: return {"error": f"ML insights generation failed: {str(e)}", "available": False} async def _advanced_customer_segmentation(self, db: Session) -> Dict[str, Any]: """ Advanced customer segmentation using ML clustering """ try: # Get customer metrics customers = db.query( KhachHang.id, KhachHang.ten, KhachHang.loai, func.count(DonHang.id).label('order_count'), func.sum(DonHang.tong_tien).label('total_spent'), func.avg(DonHang.tong_tien).label('avg_order_value'), func.max(DonHang.ngay_tao).label('last_order_date'), func.datediff(func.now(), func.max(DonHang.ngay_tao)).label('days_since_last_order') ).outerjoin(DonHang).filter( DonHang.trang_thai != TrangThaiDonHang.HUY ).group_by(KhachHang.id).having( func.count(DonHang.id) > 0 ).all() if len(customers) < 5: return {"error": "Insufficient customer data for segmentation"} # Prepare features for clustering features_data = [] for customer in customers: recency = customer.days_since_last_order or 0 frequency = customer.order_count monetary = float(customer.total_spent or 0) avg_order = float(customer.avg_order_value or 0) features_data.append([recency, frequency, monetary, avg_order]) features = np.array(features_data) # Normalize features scaler = StandardScaler() features_scaled = scaler.fit_transform(features) # Apply K-means clustering optimal_k = min(5, len(customers) // 2) kmeans = KMeans(n_clusters=optimal_k, random_state=42) cluster_labels = kmeans.fit_predict(features_scaled) # Analyze clusters clusters = {} for i in range(optimal_k): cluster_customers = [customers[j] for j in range(len(customers)) if cluster_labels[j] == i] if cluster_customers: cluster_features = features[cluster_labels == i] clusters[f"cluster_{i}"] = { "name": self._name_customer_cluster(i, cluster_features), "customer_count": len(cluster_customers), "avg_recency": float(np.mean(cluster_features[:, 0])), "avg_frequency": float(np.mean(cluster_features[:, 1])), "avg_monetary": float(np.mean(cluster_features[:, 2])), "avg_order_value": float(np.mean(cluster_features[:, 3])), "characteristics": self._describe_cluster_characteristics(cluster_features), "sample_customers": [ {"id": c.id, "name": c.ten, "total_spent": float(c.total_spent or 0)} for c in cluster_customers[:3] ] } return { "available": True, "total_customers": len(customers), "clusters": clusters, "algorithm": "K-Means", "features_used": ["Recency", "Frequency", "Monetary", "Avg Order Value"] } except Exception as e: return {"error": f"Customer segmentation failed: {str(e)}"} def _name_customer_cluster(self, cluster_id: int, features: np.ndarray) -> str: """ Dat ten cho customer cluster dua tren dac diem """ avg_recency = np.mean(features[:, 0]) avg_frequency = np.mean(features[:, 1]) avg_monetary = np.mean(features[:, 2]) if avg_frequency > 10 and avg_monetary > 5000000: # 5M VND return "VIP Champions" elif avg_frequency > 5 and avg_recency < 30: return "Loyal Customers" elif avg_recency > 90: return "At Risk" elif avg_frequency < 3: return "New/Occasional" elif avg_monetary > 2000000: # 2M VND return "High Value" else: return f"Standard Segment {cluster_id}" def _describe_cluster_characteristics(self, features: np.ndarray) -> List[str]: """ Mo ta dac diem cua cluster """ avg_recency = np.mean(features[:, 0]) avg_frequency = np.mean(features[:, 1]) avg_monetary = np.mean(features[:, 2]) avg_order_value = np.mean(features[:, 3]) characteristics = [] if avg_recency < 30: characteristics.append("Recent buyers") elif avg_recency > 90: characteristics.append("Haven't bought recently") if avg_frequency > 10: characteristics.append("Very frequent buyers") elif avg_frequency > 5: characteristics.append("Regular buyers") else: characteristics.append("Occasional buyers") if avg_monetary > 5000000: characteristics.append("High spending") elif avg_monetary > 1000000: characteristics.append("Moderate spending") else: characteristics.append("Low spending") if avg_order_value > 2000000: characteristics.append("Large order sizes") return characteristics async def _product_performance_clustering(self, db: Session) -> Dict[str, Any]: """ Product performance clustering de identify product segments """ try: # Get product performance metrics products = db.query( SanPham.id, SanPham.ten, SanPham.danh_muc, SanPham.gia_ban, func.count(ChiTietDonHang.id).label('order_count'), func.sum(ChiTietDonHang.so_luong).label('total_quantity'), func.sum(ChiTietDonHang.thanh_tien).label('total_revenue'), func.avg(ChiTietDonHang.gia_ban).label('avg_selling_price'), func.max(DonHang.ngay_tao).label('last_sold_date') ).outerjoin(ChiTietDonHang).outerjoin(DonHang).filter( DonHang.trang_thai != TrangThaiDonHang.HUY ).group_by(SanPham.id).all() if len(products) < 5: return {"error": "Insufficient product data for clustering"} # Prepare features features_data = [] for product in products: order_count = product.order_count or 0 total_quantity = product.total_quantity or 0 total_revenue = float(product.total_revenue or 0) price = float(product.gia_ban or 0) days_since_last_sold = (datetime.now() - product.last_sold_date).days if product.last_sold_date else 365 features_data.append([order_count, total_quantity, total_revenue, price, days_since_last_sold]) features = np.array(features_data) # Normalize features scaler = StandardScaler() features_scaled = scaler.fit_transform(features) # Apply K-means clustering optimal_k = min(4, len(products) // 3) kmeans = KMeans(n_clusters=optimal_k, random_state=42) cluster_labels = kmeans.fit_predict(features_scaled) # Analyze clusters clusters = {} for i in range(optimal_k): cluster_products = [products[j] for j in range(len(products)) if cluster_labels[j] == i] if cluster_products: cluster_features = features[cluster_labels == i] clusters[f"cluster_{i}"] = { "name": self._name_product_cluster(i, cluster_features), "product_count": len(cluster_products), "avg_order_count": float(np.mean(cluster_features[:, 0])), "avg_quantity_sold": float(np.mean(cluster_features[:, 1])), "avg_revenue": float(np.mean(cluster_features[:, 2])), "avg_price": float(np.mean(cluster_features[:, 3])), "avg_days_since_sold": float(np.mean(cluster_features[:, 4])), "sample_products": [ { "id": p.id, "name": p.ten, "category": p.danh_muc, "total_revenue": float(p.total_revenue or 0) } for p in cluster_products[:3] ] } return { "available": True, "total_products": len(products), "clusters": clusters, "algorithm": "K-Means", "features_used": ["Order Count", "Quantity Sold", "Revenue", "Price", "Days Since Last Sold"] } except Exception as e: return {"error": f"Product clustering failed: {str(e)}"} def _name_product_cluster(self, cluster_id: int, features: np.ndarray) -> str: """ Dat ten cho product cluster """ avg_order_count = np.mean(features[:, 0]) avg_revenue = np.mean(features[:, 2]) avg_days_since_sold = np.mean(features[:, 4]) if avg_order_count > 20 and avg_revenue > 10000000: # 10M VND return "Best Sellers" elif avg_days_since_sold > 180: return "Slow Movers" elif avg_order_count > 10: return "Popular Items" elif avg_revenue > 5000000: # 5M VND return "High Revenue" else: return f"Standard Products {cluster_id}" async def _calculate_ml_health_score(self, forecast, anomaly, segments, products) -> int: """ Tinh ML health score tong the """ score = 100 # Forecast health if forecast and forecast.get("available"): confidence = forecast.get("model_performance", {}).get("overall_confidence", 50) if confidence < 30: score -= 20 elif confidence < 50: score -= 10 # Anomaly health if anomaly and anomaly.get("available"): risk_score = anomaly.get("risk_assessment", {}).get("risk_score", 0) if risk_score > 70: score -= 30 elif risk_score > 40: score -= 15 # Segmentation health if segments and not segments.get("error"): cluster_count = len(segments.get("clusters", {})) if cluster_count < 3: score -= 10 return max(0, min(100, score)) async def _generate_comprehensive_recommendations(self, forecast, anomaly, segments, products) -> List[str]: """ Tao comprehensive AI recommendations """ recommendations = [] # Forecast-based recommendations if forecast and forecast.get("available"): predictions = forecast.get("predictions", []) if predictions: avg_predicted_orders = np.mean([p["predicted_orders"] for p in predictions]) if avg_predicted_orders > 50: recommendations.append(" High demand expected - consider increasing inventory") elif avg_predicted_orders < 10: recommendations.append(" Low demand predicted - focus on marketing campaigns") # Anomaly-based recommendations if anomaly and anomaly.get("available"): risk_level = anomaly.get("risk_assessment", {}).get("risk_level", "LOW") if risk_level == "HIGH": recommendations.append(" High risk detected - implement enhanced fraud monitoring") elif risk_level == "MEDIUM": recommendations.append(" Moderate risk - review suspicious transactions") # Segmentation-based recommendations if segments and not segments.get("error"): clusters = segments.get("clusters", {}) for cluster_name, cluster_data in clusters.items(): if "VIP" in cluster_data["name"]: recommendations.append(f" Nurture {cluster_data['customer_count']} VIP customers with exclusive offers") elif "At Risk" in cluster_data["name"]: recommendations.append(f" Re-engage {cluster_data['customer_count']} at-risk customers") # Product-based recommendations if products and not products.get("error"): clusters = products.get("clusters", {}) for cluster_name, cluster_data in clusters.items(): if "Slow Movers" in cluster_data["name"]: recommendations.append(f" Consider promotions for {cluster_data['product_count']} slow-moving products") elif "Best Sellers" in cluster_data["name"]: recommendations.append(f" Ensure adequate stock for {cluster_data['product_count']} best-selling products") if not recommendations: recommendations.append(" All ML indicators are healthy - maintain current strategies") return recommendations # Factory function def get_ml_engine() -> AdvancedMLEngine: """ Factory function de get ML engine instance """ return AdvancedMLEngine()